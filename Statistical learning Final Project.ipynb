{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd1e45f2-f391-4b20-a54d-cf15f9e7dbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FA 590 - Stock Return Prediction Project - FULL VERSION\n",
      "================================================================================\n",
      "\n",
      "[STEP 1] Loading Data...\n",
      "Data location: /Users/harshil/Desktop/Statistical Learning Final Project/return_predictability_data.csv\n",
      "Outputs will be saved to: /Users/harshil/Desktop/Statistical Learning Final Project\n",
      "Charts will be saved to: /Users/harshil/Desktop/Statistical Learning Final Project/charts\n",
      "\n",
      "Dataset Shape: (4089924, 107)\n",
      "Dataset Size: 4043.75 MB in memory\n",
      "Date Range: 1957-01-31 to 2021-11-30\n",
      "Number of unique stocks: 32655\n",
      "Number of unique dates: 779\n",
      "\n",
      "[STEP 2] Data Preprocessing...\n",
      "Number of features: 103\n",
      "\n",
      "[Converting data types...]\n",
      "Dataset shape after preprocessing: (4089903, 107)\n",
      "\n",
      "[Encoding categorical variable: sic2]\n",
      "Features after encoding: 176\n",
      "\n",
      "[STEP 3] Exploratory Data Analysis & Creating Charts...\n",
      "‚úì Chart 1: Target Distribution saved\n",
      "\n",
      "[Creating time series chart...]\n",
      "‚úì Chart 2: Returns Over Time saved\n",
      "\n",
      "[Calculating correlations...]\n",
      "‚úì Chart 3: Feature Correlations saved\n",
      "\n",
      "[Creating correlation heatmap...]\n",
      "‚úì Chart 4: Correlation Heatmap saved\n",
      "\n",
      "[Creating missing data chart...]\n",
      "‚úì Chart 5: Missing Data Pattern saved\n",
      "\n",
      "[STEP 4] Train-Validation-Test Split...\n",
      "Train: 1957-01-31 to 1995-11-30 (467 months)\n",
      "Val: 1995-12-29 to 2008-11-28 (156 months)\n",
      "Test: 2008-12-31 to 2021-11-30 (156 months)\n",
      "\n",
      "[Feature Scaling...]\n",
      "‚úì Chart 6: Data Split Visualization saved\n",
      "\n",
      "================================================================================\n",
      "[STEP 5] Model Training - ALL MODELS\n",
      "================================================================================\n",
      "\n",
      "--- LINEAR MODELS ---\n",
      "\n",
      "[1] Training OLS...\n",
      "‚úì OLS completed in 25.74s\n",
      "\n",
      "[2] Training Ridge...\n",
      "‚úì Ridge completed in 4.30s\n",
      "\n",
      "[3] Training Lasso...\n",
      "‚úì Lasso completed in 12.70s\n",
      "\n",
      "--- TREE-BASED MODELS ---\n",
      "\n",
      "[4] Training Random Forest...\n",
      "‚úì Random Forest completed in 886.65s\n",
      "\n",
      "[5] Training Gradient Boosting...\n",
      "‚úì Gradient Boosting completed in 3500.14s\n",
      "\n",
      "--- NEURAL NETWORK ---\n",
      "\n",
      "[6] Training Neural Network...\n",
      "‚úì Neural Network completed in 72.53s\n",
      "‚úì Chart 7: Neural Network Training History saved\n",
      "‚úì Chart 8: Training Times saved\n",
      "\n",
      "================================================================================\n",
      "[STEP 6] Predictive Performance Evaluation\n",
      "================================================================================\n",
      "\n",
      "[Performance Metrics]\n",
      "               Model     Dataset        R2       MSE      RMSE       MAE\n",
      "0                OLS       Train  0.031481  0.025758  0.160492  0.098387\n",
      "1                OLS  Validation -0.037880  0.038273  0.195636  0.123322\n",
      "2                OLS        Test -0.125829  0.034558  0.185897  0.116827\n",
      "3              Ridge       Train  0.031481  0.025758  0.160492  0.098387\n",
      "4              Ridge  Validation -0.037879  0.038273  0.195636  0.123321\n",
      "5              Ridge        Test -0.125827  0.034558  0.185897  0.116827\n",
      "6              Lasso       Train  0.028136  0.025847  0.160769  0.098328\n",
      "7              Lasso  Validation  0.007200  0.036611  0.191340  0.113061\n",
      "8              Lasso        Test -0.007632  0.030930  0.175868  0.099556\n",
      "9       RandomForest       Train  0.092782  0.024127  0.155330  0.094648\n",
      "10      RandomForest  Validation -0.379037  0.050854  0.225509  0.125760\n",
      "11      RandomForest        Test -0.097516  0.033689  0.183545  0.100685\n",
      "12  GradientBoosting       Train  0.104783  0.023808  0.154299  0.094576\n",
      "13  GradientBoosting  Validation -0.032525  0.038076  0.195131  0.117143\n",
      "14  GradientBoosting        Test -0.043760  0.032039  0.178993  0.098230\n",
      "15     NeuralNetwork       Train  0.025455  0.025918  0.160991  0.098821\n",
      "16     NeuralNetwork  Validation  0.004394  0.036715  0.191610  0.113440\n",
      "17     NeuralNetwork        Test -0.003312  0.030797  0.175491  0.096447\n",
      "\n",
      "‚úì Saved to '/Users/harshil/Desktop/Statistical Learning Final Project/predictive_performance_detailed.csv'\n",
      "‚úì Chart 9: R¬≤ Comparison saved\n",
      "‚úì Chart 10: MSE Comparison saved\n",
      "\n",
      "Best model by R¬≤: NeuralNetwork\n",
      "‚úì Chart 11: Actual vs Predicted saved\n",
      "\n",
      "================================================================================\n",
      "[STEP 7] Portfolio Performance Evaluation\n",
      "================================================================================\n",
      "\n",
      "[Portfolio Performance]\n",
      "               Model Dataset  Avg_Return  Volatility  Sharpe_Ratio\n",
      "0                OLS   Train    0.043329    0.088018      0.492281\n",
      "1                OLS    Test    0.016952    0.081098      0.209031\n",
      "2              Ridge   Train    0.043329    0.088018      0.492281\n",
      "3              Ridge    Test    0.016952    0.081098      0.209031\n",
      "4              Lasso   Train    0.043221    0.089198      0.484554\n",
      "5              Lasso    Test    0.034804    0.096289      0.361457\n",
      "6       RandomForest   Train    0.043411    0.114124      0.380382\n",
      "7       RandomForest    Test    0.022229    0.083369      0.266628\n",
      "8   GradientBoosting   Train    0.070770    0.125729      0.562872\n",
      "9   GradientBoosting    Test    0.038004    0.102093      0.372250\n",
      "10     NeuralNetwork   Train    0.018836    0.055576      0.338917\n",
      "11     NeuralNetwork    Test    0.014359    0.057894      0.248032\n",
      "\n",
      "‚úì Saved to '/Users/harshil/Desktop/Statistical Learning Final Project/portfolio_performance.csv'\n",
      "‚úì Chart 12: Portfolio Metrics saved\n",
      "‚úì Chart 13: Cumulative Returns saved\n",
      "\n",
      "================================================================================\n",
      "[STEP 8] Feature Importance Analysis\n",
      "================================================================================\n",
      "\n",
      "Top 20 Important Features:\n",
      "        Feature  Importance\n",
      "101  macro_svar    0.270421\n",
      "99    macro_tms    0.132814\n",
      "100   macro_dfy    0.114776\n",
      "98    macro_tbl    0.082639\n",
      "94     macro_dp    0.065365\n",
      "95     macro_ep    0.052707\n",
      "97   macro_ntis    0.049015\n",
      "7         mom1m    0.038974\n",
      "96     macro_bm    0.035397\n",
      "47   pchcapx_ia    0.027551\n",
      "90       retvol    0.026035\n",
      "5       idiovol    0.016695\n",
      "8         mom6m    0.010447\n",
      "4        dolvol    0.007187\n",
      "0         mvel1    0.005946\n",
      "9        mom12m    0.005079\n",
      "1          beta    0.004888\n",
      "6        indmom    0.004799\n",
      "2        betasq    0.004141\n",
      "104     sic2_13    0.003452\n",
      "\n",
      "‚úì Saved to '/Users/harshil/Desktop/Statistical Learning Final Project/feature_importance.csv'\n",
      "‚úì Chart 14: Feature Importance saved\n",
      "\n",
      "================================================================================\n",
      "[STEP 9] Creating Summary Charts\n",
      "================================================================================\n",
      "‚úì Chart 15: Train vs Test Comparison saved\n",
      "‚úì Chart 16: Risk-Return Tradeoff saved\n",
      "\n",
      "================================================================================\n",
      "PROJECT COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "üìä RESULTS SUMMARY:\n",
      "   Dataset: 4,089,903 observations, 176 features\n",
      "   Models trained: 6 (Linear + Tree-Based + Neural Network)\n",
      "   Test period: 156 months\n",
      "\n",
      "üìÅ FILES SAVED TO: /Users/harshil/Desktop/Statistical Learning Final Project\n",
      "   CSV Files:\n",
      "   ‚úì predictive_performance_detailed.csv\n",
      "   ‚úì portfolio_performance.csv\n",
      "   ‚úì feature_importance.csv\n",
      "\n",
      "üìà CHARTS SAVED TO: /Users/harshil/Desktop/Statistical Learning Final Project/charts\n",
      "   ‚úì 01_target_distribution.png\n",
      "   ‚úì 02_returns_over_time.png\n",
      "   ‚úì 03_feature_correlations.png\n",
      "   ‚úì 04_correlation_heatmap.png\n",
      "   ‚úì 05_missing_data_pattern.png\n",
      "   ‚úì 06_data_split.png\n",
      "   ‚úì 07_nn_training_history.png\n",
      "   ‚úì 08_training_times.png\n",
      "   ‚úì 09_r2_comparison.png\n",
      "   ‚úì 10_mse_comparison.png\n",
      "   ‚úì 11_actual_vs_predicted.png\n",
      "   ‚úì 12_portfolio_metrics.png\n",
      "   ‚úì 13_cumulative_returns.png\n",
      "   ‚úì 14_feature_importance.png\n",
      "   ‚úì 15_train_vs_test.png\n",
      "   ‚úì 16_risk_return_tradeoff.png\n",
      "\n",
      "üèÜ BEST MODEL (by R¬≤): NeuralNetwork\n",
      "   Test R¬≤: -0.003312\n",
      "\n",
      "üèÜ BEST PORTFOLIO (by Sharpe Ratio): GradientBoosting\n",
      "   Sharpe Ratio: 0.3722\n",
      "\n",
      "================================================================================\n",
      "All outputs ready for your report! üìù\n",
      "Ready to run on full 3.8GB dataset!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "FA 590 Statistical Learning in Finance - Final Project\n",
    "VERSION: Linear + Tree-Based + Neural Network Models\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from scipy import stats\n",
    "\n",
    "# TensorFlow for Neural Network\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "\n",
    "# Set style for professional charts\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FA 590 - Stock Return Prediction Project - FULL VERSION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. DATA LOADING\n",
    "# ============================================================================\n",
    "print(\"\\n[STEP 1] Loading Data...\")\n",
    "\n",
    "# UPDATE THIS PATH\n",
    "DATA_PATH = '/Users/harshil/Desktop/Statistical Learning Final Project/return_predictability_data.csv'\n",
    "\n",
    "# Get directory for saving outputs\n",
    "OUTPUT_DIR = os.path.dirname(DATA_PATH)\n",
    "CHARTS_DIR = os.path.join(OUTPUT_DIR, 'charts')\n",
    "os.makedirs(CHARTS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Data location: {DATA_PATH}\")\n",
    "print(f\"Outputs will be saved to: {OUTPUT_DIR}\")\n",
    "print(f\"Charts will be saved to: {CHARTS_DIR}\")\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(f\"\\nDataset Shape: {df.shape}\")\n",
    "print(f\"Dataset Size: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB in memory\")\n",
    "print(f\"Date Range: {df['DATE'].min()} to {df['DATE'].max()}\")\n",
    "print(f\"Number of unique stocks: {df['permno'].nunique()}\")\n",
    "print(f\"Number of unique dates: {df['DATE'].nunique()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. DATA PREPROCESSING\n",
    "# ============================================================================\n",
    "print(\"\\n[STEP 2] Data Preprocessing...\")\n",
    "\n",
    "ID_COLS = ['permno', 'DATE']\n",
    "if 'name' in df.columns:\n",
    "    ID_COLS.append('name')\n",
    "\n",
    "TARGET = 'RET'\n",
    "FEATURE_COLS = [col for col in df.columns if col not in ID_COLS + [TARGET]]\n",
    "\n",
    "print(f\"Number of features: {len(FEATURE_COLS)}\")\n",
    "\n",
    "# Convert to numeric\n",
    "print(\"\\n[Converting data types...]\")\n",
    "df[TARGET] = pd.to_numeric(df[TARGET], errors='coerce')\n",
    "\n",
    "# Handle missing values\n",
    "df = df.sort_values(['permno', 'DATE'])\n",
    "df[FEATURE_COLS] = df.groupby('permno')[FEATURE_COLS].ffill()\n",
    "\n",
    "for col in FEATURE_COLS:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "df = df.dropna(subset=[TARGET])\n",
    "print(f\"Dataset shape after preprocessing: {df.shape}\")\n",
    "\n",
    "# Handle sic2\n",
    "if 'sic2' in df.columns and 'sic2' in FEATURE_COLS:\n",
    "    print(\"\\n[Encoding categorical variable: sic2]\")\n",
    "    sic2_dummies = pd.get_dummies(df['sic2'], prefix='sic2', drop_first=True)\n",
    "    df = pd.concat([df, sic2_dummies], axis=1)\n",
    "    df = df.drop('sic2', axis=1)\n",
    "    FEATURE_COLS = [col for col in df.columns if col not in ID_COLS + [TARGET]]\n",
    "    print(f\"Features after encoding: {len(FEATURE_COLS)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. EXPLORATORY DATA ANALYSIS WITH CHARTS\n",
    "# ============================================================================\n",
    "print(\"\\n[STEP 3] Exploratory Data Analysis & Creating Charts...\")\n",
    "\n",
    "# CHART 1: Target Distribution (Multi-panel)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "axes[0, 0].hist(df[TARGET], bins=100, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[0, 0].set_xlabel('Monthly Return (RET)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Full Distribution of Stock Returns')\n",
    "axes[0, 0].axvline(df[TARGET].mean(), color='red', linestyle='--', label=f'Mean: {df[TARGET].mean():.4f}')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "axes[0, 1].hist(df[TARGET], bins=100, edgecolor='black', alpha=0.7, color='darkgreen')\n",
    "axes[0, 1].set_xlabel('Monthly Return (RET)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Returns Distribution (Zoomed: -50% to +50%)')\n",
    "axes[0, 1].set_xlim(-0.5, 0.5)\n",
    "\n",
    "axes[1, 0].boxplot(df[TARGET], vert=True)\n",
    "axes[1, 0].set_ylabel('Monthly Return')\n",
    "axes[1, 0].set_title('Boxplot of Stock Returns')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# QQ plot\n",
    "stats.probplot(df[TARGET], dist=\"norm\", plot=axes[1, 1])\n",
    "axes[1, 1].set_title('Q-Q Plot: Returns vs Normal Distribution')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CHARTS_DIR, '01_target_distribution.png'), bbox_inches='tight')\n",
    "print(\"‚úì Chart 1: Target Distribution saved\")\n",
    "plt.close()\n",
    "\n",
    "# CHART 2: Returns Over Time\n",
    "print(\"\\n[Creating time series chart...]\")\n",
    "monthly_avg = df.groupby('DATE')[TARGET].mean().reset_index()\n",
    "monthly_avg['DATE'] = pd.to_datetime(monthly_avg['DATE'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "ax.plot(monthly_avg['DATE'], monthly_avg[TARGET], linewidth=1.5, color='darkblue')\n",
    "ax.axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "ax.fill_between(monthly_avg['DATE'], monthly_avg[TARGET], alpha=0.3, color='skyblue')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Average Monthly Return')\n",
    "ax.set_title('Average Stock Returns Over Time (2009-2021)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CHARTS_DIR, '02_returns_over_time.png'), bbox_inches='tight')\n",
    "print(\"‚úì Chart 2: Returns Over Time saved\")\n",
    "plt.close()\n",
    "\n",
    "# CHART 3: Top Feature Correlations\n",
    "print(\"\\n[Calculating correlations...]\")\n",
    "numeric_cols = df[FEATURE_COLS].select_dtypes(include=[np.number]).columns[:100]\n",
    "correlations = df[numeric_cols].corrwith(df[TARGET]).abs().sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "top_20 = correlations.head(20)\n",
    "ax.barh(range(len(top_20)), top_20.values, color='teal')\n",
    "ax.set_yticks(range(len(top_20)))\n",
    "ax.set_yticklabels(top_20.index)\n",
    "ax.set_xlabel('Absolute Correlation with Returns')\n",
    "ax.set_title('Top 20 Features by Correlation with Stock Returns')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CHARTS_DIR, '03_feature_correlations.png'), bbox_inches='tight')\n",
    "print(\"‚úì Chart 3: Feature Correlations saved\")\n",
    "plt.close()\n",
    "\n",
    "# CHART 4: Correlation Heatmap\n",
    "print(\"\\n[Creating correlation heatmap...]\")\n",
    "top_features = correlations.head(15).index.tolist()\n",
    "corr_matrix = df[top_features + [TARGET]].corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "ax.set_title('Correlation Heatmap: Top 15 Features + Returns')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CHARTS_DIR, '04_correlation_heatmap.png'), bbox_inches='tight')\n",
    "print(\"‚úì Chart 4: Correlation Heatmap saved\")\n",
    "plt.close()\n",
    "\n",
    "# CHART 5: Missing Data Pattern\n",
    "print(\"\\n[Creating missing data chart...]\")\n",
    "df_original = pd.read_csv(DATA_PATH)\n",
    "missing_counts = df_original.isnull().sum().sort_values(ascending=False).head(20)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.bar(range(len(missing_counts)), missing_counts.values, color='coral')\n",
    "ax.set_xticks(range(len(missing_counts)))\n",
    "ax.set_xticklabels(missing_counts.index, rotation=45, ha='right')\n",
    "ax.set_ylabel('Number of Missing Values')\n",
    "ax.set_title('Top 20 Features with Missing Values (Before Imputation)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CHARTS_DIR, '05_missing_data_pattern.png'), bbox_inches='tight')\n",
    "print(\"‚úì Chart 5: Missing Data Pattern saved\")\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# 4. TRAIN-TEST SPLIT\n",
    "# ============================================================================\n",
    "print(\"\\n[STEP 4] Train-Validation-Test Split...\")\n",
    "\n",
    "df = df.sort_values('DATE').reset_index(drop=True)\n",
    "unique_dates = sorted(df['DATE'].unique())\n",
    "n_dates = len(unique_dates)\n",
    "\n",
    "train_end = int(0.6 * n_dates)\n",
    "val_end = int(0.8 * n_dates)\n",
    "\n",
    "train_dates = unique_dates[:train_end]\n",
    "val_dates = unique_dates[train_end:val_end]\n",
    "test_dates = unique_dates[val_end:]\n",
    "\n",
    "print(f\"Train: {train_dates[0]} to {train_dates[-1]} ({len(train_dates)} months)\")\n",
    "print(f\"Val: {val_dates[0]} to {val_dates[-1]} ({len(val_dates)} months)\")\n",
    "print(f\"Test: {test_dates[0]} to {test_dates[-1]} ({len(test_dates)} months)\")\n",
    "\n",
    "train_df = df[df['DATE'].isin(train_dates)].copy()\n",
    "val_df = df[df['DATE'].isin(val_dates)].copy()\n",
    "test_df = df[df['DATE'].isin(test_dates)].copy()\n",
    "\n",
    "X_train = train_df[FEATURE_COLS]\n",
    "y_train = train_df[TARGET]\n",
    "X_val = val_df[FEATURE_COLS]\n",
    "y_val = val_df[TARGET]\n",
    "X_test = test_df[FEATURE_COLS]\n",
    "y_test = test_df[TARGET]\n",
    "\n",
    "# Scaling\n",
    "print(\"\\n[Feature Scaling...]\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# CHART 6: Data Split Visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "split_data = pd.DataFrame({\n",
    "    'Period': ['Train', 'Validation', 'Test'],\n",
    "    'Observations': [len(train_df), len(val_df), len(test_df)],\n",
    "    'Months': [len(train_dates), len(val_dates), len(test_dates)]\n",
    "})\n",
    "\n",
    "x = np.arange(len(split_data))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, split_data['Observations'], width, label='Observations', color='steelblue')\n",
    "ax2 = ax.twinx()\n",
    "ax2.bar(x + width/2, split_data['Months'], width, label='Months', color='coral')\n",
    "\n",
    "ax.set_xlabel('Dataset Split')\n",
    "ax.set_ylabel('Number of Observations', color='steelblue')\n",
    "ax2.set_ylabel('Number of Months', color='coral')\n",
    "ax.set_title('Train-Validation-Test Split')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(split_data['Period'])\n",
    "ax.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CHARTS_DIR, '06_data_split.png'), bbox_inches='tight')\n",
    "print(\"‚úì Chart 6: Data Split Visualization saved\")\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# 5. MODEL TRAINING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[STEP 5] Model Training - ALL MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = {}\n",
    "training_times = {}\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 5.1 LINEAR MODELS\n",
    "# ---------------------------------------------------------------------------\n",
    "print(\"\\n--- LINEAR MODELS ---\")\n",
    "\n",
    "# OLS\n",
    "print(\"\\n[1] Training OLS...\")\n",
    "start = time.time()\n",
    "lr_ols = LinearRegression()\n",
    "lr_ols.fit(X_train_scaled, y_train)\n",
    "training_times['OLS'] = time.time() - start\n",
    "results['OLS'] = {\n",
    "    'train': lr_ols.predict(X_train_scaled),\n",
    "    'val': lr_ols.predict(X_val_scaled),\n",
    "    'test': lr_ols.predict(X_test_scaled)\n",
    "}\n",
    "print(f\"‚úì OLS completed in {training_times['OLS']:.2f}s\")\n",
    "\n",
    "# Ridge\n",
    "print(\"\\n[2] Training Ridge...\")\n",
    "start = time.time()\n",
    "lr_ridge = Ridge(alpha=1.0)\n",
    "lr_ridge.fit(X_train_scaled, y_train)\n",
    "training_times['Ridge'] = time.time() - start\n",
    "results['Ridge'] = {\n",
    "    'train': lr_ridge.predict(X_train_scaled),\n",
    "    'val': lr_ridge.predict(X_val_scaled),\n",
    "    'test': lr_ridge.predict(X_test_scaled)\n",
    "}\n",
    "print(f\"‚úì Ridge completed in {training_times['Ridge']:.2f}s\")\n",
    "\n",
    "# Lasso\n",
    "print(\"\\n[3] Training Lasso...\")\n",
    "start = time.time()\n",
    "lr_lasso = Lasso(alpha=0.001, max_iter=5000)\n",
    "lr_lasso.fit(X_train_scaled, y_train)\n",
    "training_times['Lasso'] = time.time() - start\n",
    "results['Lasso'] = {\n",
    "    'train': lr_lasso.predict(X_train_scaled),\n",
    "    'val': lr_lasso.predict(X_val_scaled),\n",
    "    'test': lr_lasso.predict(X_test_scaled)\n",
    "}\n",
    "print(f\"‚úì Lasso completed in {training_times['Lasso']:.2f}s\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 5.2 TREE-BASED MODELS\n",
    "# ---------------------------------------------------------------------------\n",
    "print(\"\\n--- TREE-BASED MODELS ---\")\n",
    "\n",
    "# Random Forest\n",
    "print(\"\\n[4] Training Random Forest...\")\n",
    "start = time.time()\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=50,\n",
    "    max_depth=8,\n",
    "    min_samples_split=100,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "training_times['RandomForest'] = time.time() - start\n",
    "results['RandomForest'] = {\n",
    "    'train': rf.predict(X_train),\n",
    "    'val': rf.predict(X_val),\n",
    "    'test': rf.predict(X_test)\n",
    "}\n",
    "print(f\"‚úì Random Forest completed in {training_times['RandomForest']:.2f}s\")\n",
    "\n",
    "# Gradient Boosting\n",
    "print(\"\\n[5] Training Gradient Boosting...\")\n",
    "start = time.time()\n",
    "gb = GradientBoostingRegressor(\n",
    "    n_estimators=50,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "gb.fit(X_train, y_train)\n",
    "training_times['GradientBoosting'] = time.time() - start\n",
    "results['GradientBoosting'] = {\n",
    "    'train': gb.predict(X_train),\n",
    "    'val': gb.predict(X_val),\n",
    "    'test': gb.predict(X_test)\n",
    "}\n",
    "print(f\"‚úì Gradient Boosting completed in {training_times['GradientBoosting']:.2f}s\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 5.3 NEURAL NETWORK\n",
    "# ---------------------------------------------------------------------------\n",
    "print(\"\\n--- NEURAL NETWORK ---\")\n",
    "print(\"\\n[6] Training Neural Network...\")\n",
    "start = time.time()\n",
    "\n",
    "# Build neural network\n",
    "nn = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "nn.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "          loss='mse', \n",
    "          metrics=['mae'])\n",
    "\n",
    "# Early stopping\n",
    "early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train\n",
    "history = nn.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_data=(X_val_scaled, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=512,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "training_times['NeuralNetwork'] = time.time() - start\n",
    "\n",
    "y_pred_nn_train = nn.predict(X_train_scaled, verbose=0).flatten()\n",
    "y_pred_nn_val = nn.predict(X_val_scaled, verbose=0).flatten()\n",
    "y_pred_nn_test = nn.predict(X_test_scaled, verbose=0).flatten()\n",
    "\n",
    "results['NeuralNetwork'] = {\n",
    "    'train': y_pred_nn_train,\n",
    "    'val': y_pred_nn_val,\n",
    "    'test': y_pred_nn_test\n",
    "}\n",
    "print(f\"‚úì Neural Network completed in {training_times['NeuralNetwork']:.2f}s\")\n",
    "\n",
    "# CHART 7: Neural Network Training History\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('MSE')\n",
    "axes[0].set_title('Neural Network Training: Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(history.history['mae'], label='Train MAE', linewidth=2)\n",
    "axes[1].plot(history.history['val_mae'], label='Val MAE', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].set_title('Neural Network Training: Mean Absolute Error')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CHARTS_DIR, '07_nn_training_history.png'), bbox_inches='tight')\n",
    "print(\"‚úì Chart 7: Neural Network Training History saved\")\n",
    "plt.close()\n",
    "\n",
    "# CHART 8: Training Times\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "models_list = list(training_times.keys())\n",
    "times = list(training_times.values())\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
    "ax.barh(models_list, times, color=colors)\n",
    "ax.set_xlabel('Training Time (seconds)')\n",
    "ax.set_title('Model Training Time Comparison')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "for i, v in enumerate(times):\n",
    "    ax.text(v, i, f' {v:.2f}s', va='center')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CHARTS_DIR, '08_training_times.png'), bbox_inches='tight')\n",
    "print(\"‚úì Chart 8: Training Times saved\")\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# 6. PREDICTIVE PERFORMANCE EVALUATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[STEP 6] Predictive Performance Evaluation\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def evaluate_predictions(y_true, y_pred, dataset_name, model_name):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    return {'Model': model_name, 'Dataset': dataset_name, 'R2': r2, 'MSE': mse, 'RMSE': rmse, 'MAE': mae}\n",
    "\n",
    "performance_metrics = []\n",
    "for model_name, preds in results.items():\n",
    "    performance_metrics.append(evaluate_predictions(y_train, preds['train'], 'Train', model_name))\n",
    "    performance_metrics.append(evaluate_predictions(y_val, preds['val'], 'Validation', model_name))\n",
    "    performance_metrics.append(evaluate_predictions(y_test, preds['test'], 'Test', model_name))\n",
    "\n",
    "perf_df = pd.DataFrame(performance_metrics)\n",
    "print(\"\\n[Performance Metrics]\")\n",
    "print(perf_df)\n",
    "\n",
    "perf_df.to_csv(os.path.join(OUTPUT_DIR, 'predictive_performance_detailed.csv'), index=False)\n",
    "print(f\"\\n‚úì Saved to '{OUTPUT_DIR}/predictive_performance_detailed.csv'\")\n",
    "\n",
    "# CHART 9: R-squared Comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "test_perf = perf_df[perf_df['Dataset'] == 'Test']\n",
    "x = np.arange(len(test_perf))\n",
    "ax.bar(x, test_perf['R2'], color=colors)\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('R-squared')\n",
    "ax.set_title('Model Performance: R¬≤ on Test Set (Out-of-Sample)')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(test_perf['Model'], rotation=15)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate(test_perf['R2']):\n",
    "    ax.text(i, v, f'{v:.4f}', ha='center', va='bottom')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CHARTS_DIR, '09_r2_comparison.png'), bbox_inches='tight')\n",
    "print(\"‚úì Chart 9: R¬≤ Comparison saved\")\n",
    "plt.close()\n",
    "\n",
    "# CHART 10: MSE Comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.bar(x, test_perf['MSE'], color=colors)\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Mean Squared Error')\n",
    "ax.set_title('Model Performance: MSE on Test Set (Lower is Better)')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(test_perf['Model'], rotation=15)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate(test_perf['MSE']):\n",
    "    ax.text(i, v, f'{v:.6f}', ha='center', va='bottom', fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CHARTS_DIR, '10_mse_comparison.png'), bbox_inches='tight')\n",
    "print(\"‚úì Chart 10: MSE Comparison saved\")\n",
    "plt.close()\n",
    "\n",
    "# CHART 11: Actual vs Predicted (Best Model)\n",
    "best_model = test_perf.loc[test_perf['R2'].idxmax(), 'Model']\n",
    "print(f\"\\nBest model by R¬≤: {best_model}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "axes[0].scatter(y_test, results[best_model]['test'], alpha=0.3, s=10, color='steelblue')\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[0].set_xlabel('Actual Returns')\n",
    "axes[0].set_ylabel('Predicted Returns')\n",
    "axes[0].set_title(f'Actual vs Predicted Returns: {best_model}')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "residuals = y_test - results[best_model]['test']\n",
    "axes[1].scatter(results[best_model]['test'], residuals, alpha=0.3, s=10, color='coral')\n",
    "axes[1].axhline(y=0, color='r', linestyle='--')\n",
    "axes[1].set_xlabel('Predicted Returns')\n",
    "axes[1].set_ylabel('Residuals')\n",
    "axes[1].set_title(f'Residual Plot: {best_model}')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CHARTS_DIR, '11_actual_vs_predicted.png'), bbox_inches='tight')\n",
    "print(\"‚úì Chart 11: Actual vs Predicted saved\")\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# 7. PORTFOLIO PERFORMANCE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[STEP 7] Portfolio Performance Evaluation\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def portfolio_performance(df_subset, predictions, dates_list):\n",
    "    df_temp = df_subset.copy()\n",
    "    df_temp['Prediction'] = predictions\n",
    "    monthly_returns = []\n",
    "    \n",
    "    for date in dates_list:\n",
    "        date_data = df_temp[df_temp['DATE'] == date]\n",
    "        if len(date_data) < 100:\n",
    "            continue\n",
    "        top100 = date_data.nlargest(100, 'Prediction')\n",
    "        monthly_returns.append(top100[TARGET].mean())\n",
    "    \n",
    "    return {\n",
    "        'Avg_Return': np.mean(monthly_returns),\n",
    "        'Volatility': np.std(monthly_returns),\n",
    "        'Sharpe_Ratio': np.mean(monthly_returns) / np.std(monthly_returns) if np.std(monthly_returns) > 0 else 0,\n",
    "        'N_Months': len(monthly_returns),\n",
    "        'Returns': monthly_returns\n",
    "    }\n",
    "\n",
    "portfolio_metrics = []\n",
    "portfolio_returns_dict = {}\n",
    "\n",
    "for model_name, preds in results.items():\n",
    "    port_train = portfolio_performance(train_df, preds['train'], train_dates)\n",
    "    port_test = portfolio_performance(test_df, preds['test'], test_dates)\n",
    "    \n",
    "    portfolio_metrics.append({\n",
    "        'Model': model_name, \n",
    "        'Dataset': 'Train',\n",
    "        'Avg_Return': port_train['Avg_Return'],\n",
    "        'Volatility': port_train['Volatility'],\n",
    "        'Sharpe_Ratio': port_train['Sharpe_Ratio']\n",
    "    })\n",
    "    portfolio_metrics.append({\n",
    "        'Model': model_name,\n",
    "        'Dataset': 'Test',\n",
    "        'Avg_Return': port_test['Avg_Return'],\n",
    "        'Volatility': port_test['Volatility'],\n",
    "        'Sharpe_Ratio': port_test['Sharpe_Ratio']\n",
    "    })\n",
    "    \n",
    "    portfolio_returns_dict[model_name] = port_test['Returns']\n",
    "\n",
    "port_df = pd.DataFrame(portfolio_metrics)\n",
    "print(\"\\n[Portfolio Performance]\")\n",
    "print(port_df)\n",
    "\n",
    "port_df.to_csv(os.path.join(OUTPUT_DIR, 'portfolio_performance.csv'), index=False)\n",
    "print(f\"\\n‚úì Saved to '{OUTPUT_DIR}/portfolio_performance.csv'\")\n",
    "\n",
    "# CHART 12: Portfolio Metrics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "port_test = port_df[port_df['Dataset'] == 'Test']\n",
    "\n",
    "axes[0].barh(port_test['Model'], port_test['Avg_Return'], color=colors)\n",
    "axes[0].set_xlabel('Average Monthly Return')\n",
    "axes[0].set_title('Portfolio Average Return (Test Set)')\n",
    "axes[0].axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "axes[1].barh(port_test['Model'], port_test['Volatility'], color=colors)\n",
    "axes[1].set_xlabel('Volatility (Std Dev)')\n",
    "axes[1].set_title('Portfolio Volatility (Test Set)')\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "axes[2].barh(port_test['Model'], port_test['Sharpe_Ratio'], color=colors)\n",
    "axes[2].set_xlabel('Sharpe Ratio')\n",
    "axes[2].set_title('Portfolio Sharpe Ratio (Test Set)')\n",
    "axes[2].axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "axes[2].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CHARTS_DIR, '12_portfolio_metrics.png'), bbox_inches='tight')\n",
    "print(\"‚úì Chart 12: Portfolio Metrics saved\")\n",
    "plt.close()\n",
    "\n",
    "# CHART 13: Cumulative Returns\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "for model_name, returns in portfolio_returns_dict.items():\n",
    "    cumulative = np.cumprod(1 + np.array(returns)) - 1\n",
    "    ax.plot(range(len(cumulative)), cumulative, label=model_name, linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Months (Test Period)')\n",
    "ax.set_ylabel('Cumulative Return')\n",
    "ax.set_title('Cumulative Portfolio Returns Over Time (Test Period)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(y=0, color='black', linestyle='--', linewidth=0.8)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CHARTS_DIR, '13_cumulative_returns.png'), bbox_inches='tight')\n",
    "print(\"‚úì Chart 13: Cumulative Returns saved\")\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# 8. FEATURE IMPORTANCE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[STEP 8] Feature Importance Analysis\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "rf_importance = pd.DataFrame({\n",
    "    'Feature': FEATURE_COLS,\n",
    "    'Importance': rf.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 Important Features:\")\n",
    "print(rf_importance.head(20))\n",
    "\n",
    "rf_importance.to_csv(os.path.join(OUTPUT_DIR, 'feature_importance.csv'), index=False)\n",
    "print(f\"\\n‚úì Saved to '{OUTPUT_DIR}/feature_importance.csv'\")\n",
    "\n",
    "# CHART 14: Feature Importance\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "top_20_imp = rf_importance.head(20)\n",
    "ax.barh(range(len(top_20_imp)), top_20_imp['Importance'], color='forestgreen')\n",
    "ax.set_yticks(range(len(top_20_imp)))\n",
    "ax.set_yticklabels(top_20_imp['Feature'])\n",
    "ax.set_xlabel('Importance Score')\n",
    "ax.set_title('Top 20 Most Important Features (Random Forest)')\n",
    "ax.invert_yaxis()\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CHARTS_DIR, '14_feature_importance.png'), bbox_inches='tight')\n",
    "print(\"‚úì Chart 14: Feature Importance saved\")\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# 9. MODEL COMPARISON SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"[STEP 9] Creating Summary Charts\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# CHART 15: Train vs Test Performance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "train_perf = perf_df[perf_df['Dataset'] == 'Train']\n",
    "test_perf_chart = perf_df[perf_df['Dataset'] == 'Test']\n",
    "\n",
    "x = np.arange(len(train_perf))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, train_perf['R2'], width, label='Train', color='steelblue', alpha=0.8)\n",
    "axes[0].bar(x + width/2, test_perf_chart['R2'], width, label='Test', color='coral', alpha=0.8)\n",
    "axes[0].set_xlabel('Model')\n",
    "axes[0].set_ylabel('R-squared')\n",
    "axes[0].set_title('R¬≤ Comparison: Train vs Test (Overfitting Check)')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(train_perf['Model'], rotation=15, ha='right')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "axes[1].bar(x - width/2, train_perf['MSE'], width, label='Train', color='steelblue', alpha=0.8)\n",
    "axes[1].bar(x + width/2, test_perf_chart['MSE'], width, label='Test', color='coral', alpha=0.8)\n",
    "axes[1].set_xlabel('Model')\n",
    "axes[1].set_ylabel('MSE')\n",
    "axes[1].set_title('MSE Comparison: Train vs Test')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(train_perf['Model'], rotation=15, ha='right')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CHARTS_DIR, '15_train_vs_test.png'), bbox_inches='tight')\n",
    "print(\"‚úì Chart 15: Train vs Test Comparison saved\")\n",
    "plt.close()\n",
    "\n",
    "# CHART 16: Risk-Return Tradeoff\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "port_test_plot = port_df[port_df['Dataset'] == 'Test']\n",
    "\n",
    "colors_map = {'OLS': '#1f77b4', 'Ridge': '#ff7f0e', 'Lasso': '#2ca02c', \n",
    "              'RandomForest': '#d62728', 'GradientBoosting': '#9467bd',\n",
    "              'NeuralNetwork': '#8c564b'}\n",
    "\n",
    "for idx, row in port_test_plot.iterrows():\n",
    "    ax.scatter(row['Volatility'], row['Avg_Return'], \n",
    "              s=300, alpha=0.7, color=colors_map[row['Model']], \n",
    "              label=row['Model'], edgecolors='black', linewidth=1.5)\n",
    "\n",
    "ax.set_xlabel('Portfolio Volatility (Risk)')\n",
    "ax.set_ylabel('Average Return')\n",
    "ax.set_title('Risk-Return Tradeoff: Portfolio Performance')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "ax.axhline(y=0, color='red', linestyle='--', linewidth=0.8, alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CHARTS_DIR, '16_risk_return_tradeoff.png'), bbox_inches='tight')\n",
    "print(\"‚úì Chart 16: Risk-Return Tradeoff saved\")\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# 10. FINAL SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROJECT COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä RESULTS SUMMARY:\")\n",
    "print(f\"   Dataset: {df.shape[0]:,} observations, {len(FEATURE_COLS)} features\")\n",
    "print(f\"   Models trained: {len(results)} (Linear + Tree-Based + Neural Network)\")\n",
    "print(f\"   Test period: {len(test_dates)} months\")\n",
    "\n",
    "print(f\"\\nüìÅ FILES SAVED TO: {OUTPUT_DIR}\")\n",
    "print(\"   CSV Files:\")\n",
    "print(\"   ‚úì predictive_performance_detailed.csv\")\n",
    "print(\"   ‚úì portfolio_performance.csv\")\n",
    "print(\"   ‚úì feature_importance.csv\")\n",
    "\n",
    "print(f\"\\nüìà CHARTS SAVED TO: {CHARTS_DIR}\")\n",
    "charts_list = [\n",
    "    \"01_target_distribution.png\",\n",
    "    \"02_returns_over_time.png\",\n",
    "    \"03_feature_correlations.png\",\n",
    "    \"04_correlation_heatmap.png\",\n",
    "    \"05_missing_data_pattern.png\",\n",
    "    \"06_data_split.png\",\n",
    "    \"07_nn_training_history.png\",\n",
    "    \"08_training_times.png\",\n",
    "    \"09_r2_comparison.png\",\n",
    "    \"10_mse_comparison.png\",\n",
    "    \"11_actual_vs_predicted.png\",\n",
    "    \"12_portfolio_metrics.png\",\n",
    "    \"13_cumulative_returns.png\",\n",
    "    \"14_feature_importance.png\",\n",
    "    \"15_train_vs_test.png\",\n",
    "    \"16_risk_return_tradeoff.png\"\n",
    "]\n",
    "\n",
    "for i, chart in enumerate(charts_list, 1):\n",
    "    print(f\"   ‚úì {chart}\")\n",
    "\n",
    "print(f\"\\nüèÜ BEST MODEL (by R¬≤): {best_model}\")\n",
    "best_r2 = test_perf.loc[test_perf['R2'].idxmax(), 'R2']\n",
    "print(f\"   Test R¬≤: {best_r2:.6f}\")\n",
    "\n",
    "best_sharpe_idx = port_df[port_df['Dataset'] == 'Test']['Sharpe_Ratio'].idxmax()\n",
    "best_sharpe_model = port_df.loc[best_sharpe_idx, 'Model']\n",
    "best_sharpe = port_df.loc[best_sharpe_idx, 'Sharpe_Ratio']\n",
    "print(f\"\\nüèÜ BEST PORTFOLIO (by Sharpe Ratio): {best_sharpe_model}\")\n",
    "print(f\"   Sharpe Ratio: {best_sharpe:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c9ca65-2ce6-4519-be51-8759b7524ac3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
